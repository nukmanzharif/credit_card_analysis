{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Transaction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "The first step is to import the necessary libraries. We will be using PySpark for this project. PySpark is a Python library that allows us to use Spark for big data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, from_unixtime\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create new spark session\n",
    "\n",
    "\n",
    "#### Configuration Details\n",
    "\n",
    "- **Application Name:** Sets the name of the Spark application to \"CreditCardTransactionAnalysis\". This name is used for identification in the Spark UI and logs.\n",
    "\n",
    "- **Driver Memory:** Allocates 4 GB of memory for the Spark driver process. This is crucial for managing the application's execution and data.\n",
    "\n",
    "- **Executor Memory:** Allocates 4 GB of memory for each executor process. This memory is used for executing tasks and storing data.\n",
    "\n",
    "- **Python Worker Memory:** Allocates 1 GB of memory for each Python worker process. This is important for executing Python code efficiently.\n",
    "\n",
    "- **Python Worker Timeout:** Sets the timeout for Python worker processes to 120 seconds. This helps manage unresponsive workers by restarting them if they exceed this limit.\n",
    "\n",
    "- **Network Timeout:** Sets the network timeout to 120 seconds. This ensures that network operations between the driver and executors do not hang indefinitely.\n",
    "\n",
    "- **Executor Heartbeat Interval:** Sets the heartbeat interval for executors to 60 seconds. This helps the driver monitor the health of executors.\n",
    "\n",
    "- **Shuffle Partitions:** Sets the number of partitions for shuffling data during operations like joins and aggregations to 4. This helps optimize performance by controlling parallelism.\n",
    "\n",
    "- **Master URL:** Specifies that Spark should run locally with as many worker threads as there are logical cores on the machine. This is useful for development and testing.\n",
    "\n",
    "- **Get or Create Session:** Creates a new Spark session or retrieves an existing one. This ensures efficient resource management by reusing sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop existing session if any. I used this to retry configure spark session if any error occurs without restarting kernel\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new session with updated configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardTransactionAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"1g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"120\") \\\n",
    "    .config(\"spark.network.timeout\", \"120\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "df = spark.read.json(\"cc_sample_transaction.json\")\n",
    "\n",
    "# Display schema and sample data\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nSample Data:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Cleaning\n",
    "\n",
    "- Masking Credit Card Numbers. The first step in cleaning the data is to mask the credit card numbers. This is done by replacing the digits with a mask character. This function masks credit card numbers to protect sensitive information. It replaces all but the last four digits with \"XXXX\".\n",
    "\n",
    "- Registering the UDF. UDFs enable custom transformations on DataFrame columns, making it possible to apply the masking logic to entire columns of credit card numbers efficiently.\n",
    "\n",
    "- Converting Timestamps Columns. This function converts timestamp columns to a specified timezone (UTC+8) and creates a new column with the converted timestamps.\n",
    "\n",
    "- Extracting name. This function extracts the first and last names from the full name column from the `person_name` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask credit card numbers\n",
    "def mask_cc_num(cc_num):\n",
    "    if cc_num is None or len(cc_num) < 4:\n",
    "        return \"XXXX-XXXX-XXXX-XXXX\"\n",
    "    else:\n",
    "        return \"XXXX-XXXX-XXXX-\" + str(cc_num)[-4:]\n",
    "\n",
    "# Register UDF\n",
    "mask_udf = udf(mask_cc_num, StringType())\n",
    "\n",
    "# Convert timestamp columns to UTC+8\n",
    "def convert_timestamp(df, col_name):\n",
    "    return df.withColumn(\n",
    "        f\"{col_name}_converted\",\n",
    "        (F.to_timestamp(F.col(col_name), 'yyyy-MM-dd HH:mm:ss') + F.expr('INTERVAL 8 HOURS')).cast(TimestampType())\n",
    "    )\n",
    "\n",
    "# Extract first and last names from 'person_name'\n",
    "def extract_names(df):\n",
    "    name_split = split(F.col('person_name'), ',\\s*')\n",
    "    df = df.withColumn('first', name_split.getItem(0)) \\\n",
    "           .withColumn('last', name_split.getItem(1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Applying Data Transformations\n",
    "\n",
    "- Start with flattening the nested JSON structure by selecting relevant fields and extracting values from the `personal_detail` JSON object.\n",
    "\n",
    "- Extract the first and last names from the `personal_detail` JSON object.\n",
    "\n",
    "- Masking personally indentifiable information (PII) by replacing sensitive data with asterisks.\n",
    "\n",
    "- Dropping sensitive columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # First, let's check the structure of personal_detail\n",
    "    print(\"Sample of personal_detail structure:\")\n",
    "    df.select('personal_detail').show(2, truncate=False)\n",
    "    \n",
    "    # Flatten the JSON data with proper parsing\n",
    "    df_flat = df.select(\n",
    "        'Unnamed: 0',\n",
    "        'trans_date_trans_time',\n",
    "        'cc_num',\n",
    "        'merchant',\n",
    "        'category',\n",
    "        'amt',\n",
    "        F.get_json_object('personal_detail', '$.person_name').alias('person_name'),\n",
    "        F.get_json_object('personal_detail', '$.gender').alias('gender'),\n",
    "        F.get_json_object('personal_detail', '$.street').alias('street'),\n",
    "        F.get_json_object('personal_detail', '$.city').alias('city'),\n",
    "        F.get_json_object('personal_detail', '$.state').alias('state'),\n",
    "        F.get_json_object('personal_detail', '$.zip').alias('zip'),\n",
    "        F.get_json_object('personal_detail', '$.lat').alias('lat'),\n",
    "        F.get_json_object('personal_detail', '$.long').alias('long'),\n",
    "        F.get_json_object('personal_detail', '$.city_pop').alias('city_pop'),\n",
    "        F.get_json_object('personal_detail', '$.job').alias('job'),\n",
    "        F.get_json_object('personal_detail', '$.dob').alias('dob'),\n",
    "        'trans_num',\n",
    "        'merch_lat',\n",
    "        'merch_long',\n",
    "        'is_fraud',\n",
    "        'merch_zipcode',\n",
    "        'merch_last_update_time',\n",
    "        'merch_eff_time',\n",
    "        'cc_bic'\n",
    "    )\n",
    "\n",
    "    # Extract first and last names from person_name\n",
    "    df_flat = df_flat.withColumn(\n",
    "        'first',\n",
    "        F.when(\n",
    "            F.col('person_name').contains(','),\n",
    "            F.trim(F.split('person_name', ',').getItem(0))\n",
    "        ).otherwise(F.lit('UNKNOWN'))\n",
    "    ).withColumn(\n",
    "        'last',\n",
    "        F.when(\n",
    "            F.col('person_name').contains(','),\n",
    "            F.trim(F.split('person_name', ',').getItem(1))\n",
    "        ).otherwise(F.lit('UNKNOWN'))\n",
    "    )\n",
    "\n",
    "    # Convert timestamp columns to UTC+8\n",
    "    timestamp_cols = ['trans_date_trans_time', 'merch_last_update_time', 'merch_eff_time']\n",
    "    for col_name in timestamp_cols:\n",
    "        df_flat = df_flat.withColumn(\n",
    "            f\"{col_name}_converted\",\n",
    "            F.to_timestamp(F.col(col_name), 'yyyy-MM-dd HH:mm:ss')\n",
    "        ).withColumn(\n",
    "            f\"{col_name}_converted\",\n",
    "            F.from_utc_timestamp(F.col(f\"{col_name}_converted\"), \"UTC+8\")\n",
    "        )\n",
    "\n",
    "    # Mask PII (credit card numbers)\n",
    "    df_flat = df_flat.withColumn(\n",
    "        'cc_num_masked',\n",
    "        F.when(F.length('cc_num') >= 4,\n",
    "              F.concat(F.lit('XXXX-XXXX-XXXX-'), F.substring('cc_num', -4, 4)))\n",
    "        .otherwise(F.lit('XXXX-XXXX-XXXX-XXXX'))\n",
    "    )\n",
    "\n",
    "    # Cast numeric columns\n",
    "    df_flat = df_flat \\\n",
    "        .withColumn('amt', F.col('amt').cast('double')) \\\n",
    "        .withColumn('lat', F.col('lat').cast('double')) \\\n",
    "        .withColumn('long', F.col('long').cast('double')) \\\n",
    "        .withColumn('city_pop', F.col('city_pop').cast('integer')) \\\n",
    "        .withColumn('merch_lat', F.col('merch_lat').cast('double')) \\\n",
    "        .withColumn('merch_long', F.col('merch_long').cast('double')) \\\n",
    "        .withColumn('is_fraud', F.col('is_fraud').cast('integer'))\n",
    "\n",
    "    # Drop original columns that have been transformed\n",
    "    df_flat = df_flat.drop('person_name', 'cc_num')\n",
    "\n",
    "    # Rename masked credit card column\n",
    "    df_flat = df_flat.withColumnRenamed('cc_num_masked', 'cc_num')\n",
    "\n",
    "    # Show the resulting schema\n",
    "    print(\"\\nResulting Schema:\")\n",
    "    df_flat.printSchema()\n",
    "\n",
    "    # Show sample of transformed data\n",
    "    print(\"\\nSample of transformed data:\")\n",
    "    df_flat.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in transformation: {str(e)}\")\n",
    "    \n",
    "    # Additional debugging information\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(\"Original Schema:\")\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Inspection and Cleaning\n",
    "\n",
    "This cell focuses on inspecting and cleaning the dataset to ensure data quality before analysis. It identifies and handles non-numeric values, invalid entries, and casts columns to appropriate data types.\n",
    "\n",
    "- Identifying Non-Numeric 'amt' Values. Identifies entries in the `amt` column that are non-numeric or null. Ensures data quality by identifying invalid entries before analysis.\n",
    "\n",
    "- Displaying Sample Non-Numeric 'amt' Entries. Displays a sample of non-numeric entries for inspection. Helps understand the nature of invalid entries.\n",
    "\n",
    "- Removing Rows with Non-Numeric 'amt' Values. Filters out rows with non-numeric or null values in the `amt` column. Ensures subsequent analyses are based on valid numeric data.\n",
    "\n",
    "- Identifying Invalid 'is_fraud' Values. Identifies entries in the `is_fraud` column that are not '0' or '1' or are null. Ensures valid binary indicators for accurate fraud analysis.\n",
    "\n",
    "- Displaying Sample Invalid 'is_fraud' Entries. Displays a sample of invalid entries for inspection. Helps understand the nature of invalid entries.\n",
    "\n",
    "- Removing Rows with Invalid 'is_fraud' Values. Filters out rows with invalid values in the `is_fraud` column. Ensures subsequent analyses are based on valid binary indicators.\n",
    "\n",
    "- Casting Columns to Appropriate Data Types. Casts various columns to their appropriate data types. Ensures accurate calculations and analyses.\n",
    "\n",
    "- Dropping Duplicates and Handling Nulls. Removes duplicate rows and handles null values. Ensures data quality and avoids potential errors in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle non-numeric 'amt' values\n",
    "non_numeric_amt = df_flat.filter(~F.col('amt').rlike('^\\d+\\.?\\d*$') | F.col('amt').isNull())\n",
    "non_numeric_amt_count = non_numeric_amt.count()\n",
    "print(f\"Number of non-numeric 'amt' entries: {non_numeric_amt_count}\")\n",
    "\n",
    "if non_numeric_amt_count > 0:\n",
    "    print(\"Sample non-numeric 'amt' entries:\")\n",
    "    non_numeric_amt.select('amt').show(5)\n",
    "\n",
    "# Remove rows with non-numeric 'amt'\n",
    "df_flat = df_flat.filter(F.col('amt').rlike('^\\d+\\.?\\d*$') & F.col('amt').isNotNull())\n",
    "\n",
    "# Identify and handle invalid 'is_fraud' values\n",
    "non_numeric_is_fraud = df_flat.filter(~F.col('is_fraud').isin('0', '1') | F.col('is_fraud').isNull())\n",
    "non_numeric_is_fraud_count = non_numeric_is_fraud.count()\n",
    "print(f\"Number of invalid 'is_fraud' entries: {non_numeric_is_fraud_count}\")\n",
    "\n",
    "if non_numeric_is_fraud_count > 0:\n",
    "    print(\"Sample invalid 'is_fraud' entries:\")\n",
    "    non_numeric_is_fraud.select('is_fraud').show(5)\n",
    "\n",
    "# Remove rows with invalid 'is_fraud' values\n",
    "df_flat = df_flat.filter(F.col('is_fraud').isin('0', '1') & F.col('is_fraud').isNotNull())\n",
    "\n",
    "# Cast columns to appropriate data types\n",
    "df_cleaned = df_flat \\\n",
    "    .withColumn('amt', F.col('amt').cast(DoubleType())) \\\n",
    "    .withColumn('is_fraud', F.col('is_fraud').cast(IntegerType())) \\\n",
    "    .withColumn('trans_date_trans_time_converted', F.col('trans_date_trans_time_converted').cast(TimestampType())) \\\n",
    "    .withColumn('merch_last_update_time_converted', F.col('merch_last_update_time_converted').cast(TimestampType())) \\\n",
    "    .withColumn('merch_eff_time_converted', F.col('merch_eff_time_converted').cast(TimestampType())) \\\n",
    "    .withColumn('lat', F.col('lat').cast(DoubleType())) \\\n",
    "    .withColumn('long', F.col('long').cast(DoubleType())) \\\n",
    "    .withColumn('city_pop', F.col('city_pop').cast(IntegerType())) \\\n",
    "    .withColumn('merch_lat', F.col('merch_lat').cast(DoubleType())) \\\n",
    "    .withColumn('merch_long', F.col('merch_long').cast(DoubleType()))\n",
    "\n",
    "# Drop duplicates and handle nulls\n",
    "df_cleaned = df_cleaned.dropDuplicates()\n",
    "df_cleaned = df_cleaned.dropna(subset=['amt', 'is_fraud', 'trans_date_trans_time_converted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Basic Analysis\n",
    "- Caching the DataFrame. Caching is useful when the DataFrame will be reused multiple times, as it avoids recomputation and speeds up access.\n",
    "\n",
    "- Basic Dataframe information. Providing an overview of the DataFrame structure helps users understand the data they are working with.\n",
    "\n",
    "- Total number of transactions. Knowing the total number of records is essential for understanding the dataset's size and for further analysis.\n",
    "\n",
    "- Amount statistics. Calculating the minimum, maximum, and average amount of transactions helps understand the distribution of spending. These statistics provide insights into the distribution of transaction amounts, which is crucial for understanding spending patterns.\n",
    "\n",
    "- Fraud Statistics. Understanding the distribution of fraud cases is essential for evaluating the effectiveness of fraud detection measures and for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the DataFrame\n",
    "df_cleaned = df_cleaned.cache()\n",
    "\n",
    "# Basic DataFrame info\n",
    "print(\"Basic DataFrame Information:\")\n",
    "print(\"Number of columns:\", len(df_cleaned.columns))\n",
    "print(\"Column names:\", df_cleaned.columns)\n",
    "\n",
    "# Total number of transactions\n",
    "total_transactions = df_cleaned.count()\n",
    "print(f\"\\nTotal number of transactions: {total_transactions}\")\n",
    "\n",
    "# Amount statistics\n",
    "amount_stats = df_cleaned.agg(\n",
    "    F.count('amt').alias('count'),\n",
    "    F.round(F.min('amt'), 2).alias('min_amount'),\n",
    "    F.round(F.max('amt'), 2).alias('max_amount'),\n",
    "    F.round(F.avg('amt'), 2).alias('avg_amount'),\n",
    "    F.round(F.stddev('amt'), 2).alias('stddev_amount')\n",
    ")\n",
    "\n",
    "print(\"\\nAmount Statistics:\")\n",
    "amount_stats.show()\n",
    "\n",
    "# Fraud statistics\n",
    "fraud_stats = df_cleaned.groupBy('is_fraud').count()\n",
    "print(\"\\nFraud Statistics:\")\n",
    "fraud_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualization: Transaction Amount Distribution\n",
    "\n",
    "- Sampling Data for Plotting. Sampling is useful for large datasets to improve performance and reduce memory usage during visualization.\n",
    "\n",
    "- Visualizing the distribution of transaction amounts using a histogram. The histogram helps us understand the distribution of transaction amounts and identify any outliers or unusual patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample for plotting\n",
    "sample_fraction = 0.05  # Adjust the fraction as needed\n",
    "pandas_df = df_cleaned.select('amt').sample(False, sample_fraction, seed=1).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=pandas_df, x='amt', bins=50, kde=True)\n",
    "plt.title('Distribution of Transaction Amounts')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Visualization: Fraud Distribution\n",
    "\n",
    "- Grouping by Fraud Indicator. Understanding the distribution of fraud cases is essential for evaluating the effectiveness of fraud detection measures.\n",
    "\n",
    "- Creating a pie chart to visualize the distribution of fraud cases. A pie chart provides a clear visual representation of the proportion of fraudulent vs. legitimate transactions, making it easy to understand the overall fraud landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud distribution\n",
    "fraud_df = df_cleaned.groupBy('is_fraud').count().toPandas()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "labels = ['Legitimate', 'Fraudulent']\n",
    "sizes = fraud_df['count']\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['#66b3ff','#ff6666'], startangle=140)\n",
    "plt.title('Distribution of Fraudulent vs Legitimate Transactions')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Visualization: Geographic Distribution\n",
    "- Sampling Data for Geographic Analysis. This line takes a random sample of 1% of the merch_lat (merchant latitude) and merch_long (merchant longitude) columns from the cleaned DataFrame for visualization. Sampling is useful for large datasets to improve performance and reduce memory usage during visualization.\n",
    "\n",
    "- Creating Scatter Plot. Visualizing the geographic distribution helps identify patterns in transaction locations and can reveal areas with high transaction density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling data for geographic analysis\n",
    "geo_sample_fraction = 0.01  # Adjust the fraction as needed\n",
    "geo_df = df_cleaned.select('merch_lat', 'merch_long').sample(False, geo_sample_fraction, seed=1).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(geo_df['merch_long'], geo_df['merch_lat'], alpha=0.5, s=10)\n",
    "plt.title('Geographic Distribution of Transactions')\n",
    "plt.xlabel('Merchant Longitude')\n",
    "plt.ylabel('Merchant Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save Large Dataset\n",
    "\n",
    "- **Purpose:** Saves the cleaned dataset to a file for further analysis.\n",
    "\n",
    "- **Significance:** This step allows you to save the cleaned dataset for future reference or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting data export process...\")\n",
    "    \n",
    "    # Get total count\n",
    "    total_records = df_cleaned.count()\n",
    "    print(f\"Total records to process: {total_records}\")\n",
    "    \n",
    "    # Calculate number of chunks needed\n",
    "    CHUNK_SIZE = 50000  # Increased chunk size\n",
    "    num_chunks = (total_records // CHUNK_SIZE) + 1\n",
    "    print(f\"Will process data in {num_chunks} chunks\")\n",
    "    \n",
    "    # Prepare output file\n",
    "    output_path = \"./cleaned_transactions_final.csv\"\n",
    "    \n",
    "    # Process first chunk with headers\n",
    "    print(\"\\nProcessing chunk 1...\")\n",
    "    first_chunk = df_cleaned.limit(CHUNK_SIZE).toPandas()\n",
    "    first_chunk.to_csv(output_path, index=False, mode='w')\n",
    "    records_processed = len(first_chunk)\n",
    "    print(f\"Processed {records_processed:,} records\")\n",
    "    \n",
    "    # Process remaining chunks\n",
    "    for i in range(1, num_chunks):\n",
    "        print(f\"\\nProcessing chunk {i+1} of {num_chunks}...\")\n",
    "        \n",
    "        next_chunk = df_cleaned.offset(i * CHUNK_SIZE).limit(CHUNK_SIZE).toPandas()\n",
    "        next_chunk.to_csv(output_path, index=False, mode='a', header=False)\n",
    "        \n",
    "        records_processed += len(next_chunk)\n",
    "        print(f\"Total processed: {records_processed:,} of {total_records:,} records\")\n",
    "        \n",
    "        # Optional: Add progress percentage\n",
    "        progress = (records_processed / total_records) * 100\n",
    "        print(f\"Progress: {progress:.1f}%\")\n",
    "    \n",
    "    print(\"\\nExport completed successfully!\")\n",
    "    print(f\"Output file: {output_path}\")\n",
    "    print(f\"Total records saved: {records_processed:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during export: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nAttempting to save a sample of the data...\")\n",
    "        sample_size = 10000\n",
    "        print(f\"Saving {sample_size:,} records as sample...\")\n",
    "        \n",
    "        sample_df = df_cleaned.limit(sample_size).toPandas()\n",
    "        sample_path = \"./cleaned_transactions_sample.csv\"\n",
    "        sample_df.to_csv(sample_path, index=False)\n",
    "        \n",
    "        print(f\"Sample saved successfully to: {sample_path}\")\n",
    "        print(f\"Sample size: {len(sample_df):,} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sample: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Stop Spark session\n",
    "    print(\"\\nStopping Spark session...\")\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
